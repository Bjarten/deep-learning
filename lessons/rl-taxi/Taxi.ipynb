{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the __init__() method to define any needed instance variables. Currently, we define the number of actions available to the agent (nA) and initialize the action values (Q) to an empty dictionary of arrays. Feel free to add more instance variables; for example, you may find it useful to define the value of epsilon if the agent uses an epsilon-greedy policy for selecting actions.\n",
    "\n",
    "The select_action() method accepts the environment state as input and returns the agent's choice of action. The default code that we have provided randomly selects an action.\n",
    "\n",
    "The step() method accepts a (state, action, reward, next_state) tuple as input, along with the done variable, which is True if the episode has ended. The default code (which you should certainly change!) increments the action value of the previous state-action pair by 1. You should change this method to use the sampled tuple of experience to update the agent's knowledge of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\" Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - nA: number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "        self.epsilon = 0.00005\n",
    "        self.alpha = 0.9\n",
    "        self.gamma = 0.9\n",
    "        self.policy_s = None\n",
    "         \n",
    "    def update_Q(self, Qsa, Qsa_next, reward):\n",
    "        \"\"\" updates the action-value function estimate using the most recent time step \"\"\"\n",
    "        return Qsa + (self.alpha * (reward + (self.gamma * Qsa_next) - Qsa))\n",
    "\n",
    "    def epsilon_greedy_probs(self, env, Q_s):\n",
    "        \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "        \n",
    "#         epsilon=1.0/i_episode\n",
    "#         if eps is not None:\n",
    "#             epsilon=eps\n",
    "        \n",
    "        policy_s = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        policy_s[np.argmax(Q_s)] = 1 - self.epsilon + (self.epsilon / self.nA)\n",
    "        return policy_s\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\" Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        - action: an integer, compatible with the task's action space\n",
    "        \"\"\"\n",
    "        # get epsilon-greedy action probabilities\n",
    "        self.policy_s = self.epsilon_greedy_probs(env, self.Q[state])\n",
    "        # pick next action A'\n",
    "        next_action = np.random.choice(np.arange(self.nA), p=self.policy_s)\n",
    "        \n",
    "        return next_action\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the previous state of the environment\n",
    "        - action: the agent's previous choice of action\n",
    "        - reward: last reward received\n",
    "        - next_state: the current state of the environment\n",
    "        - done: whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        self.Q[state][action] = self.update_Q(self.Q[state][action],np.dot(self.Q[next_state], self.policy_s), reward) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20000/20000 || Best average reward 9.395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "agent = Agent()\n",
    "avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
